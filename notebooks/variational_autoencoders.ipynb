{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 12:26:48.430107: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-28 12:26:48.498514: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/urbans50/micromamba/envs/deeputi/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Layer, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from albumentations.core.serialization import load as load_albumentations_transform\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "try:\n",
    "    from tensorflow.keras.optimizers.legacy import Adam\n",
    "except ImportError:\n",
    "    from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from src.data.image_datasets import ImageRandomDataset\n",
    "from src.utils.config import read_json_config\n",
    "from src.models.trainer import VAETrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder(\n",
    "    img_height,\n",
    "    img_width,\n",
    "    latent_dim=2,\n",
    "    ):\n",
    "\n",
    "    input_layer = Input(shape=(img_height, img_width, 1))\n",
    "\n",
    "    conv1 = Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")\n",
    "    conv2 = Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")\n",
    "\n",
    "    dense = Dense(16, activation=\"relu\")\n",
    "\n",
    "    dense_mean = Dense(latent_dim, name=\"z_mean\")\n",
    "    dense_log_var = Dense(latent_dim, name=\"z_log_var\")\n",
    "\n",
    "    flatten = Flatten()\n",
    "    sampling = Sampling()\n",
    "\n",
    "    x = conv2(conv1(input_layer))\n",
    "    x = dense(flatten(x))\n",
    "\n",
    "    z_mean, z_log_var = dense_mean(x), dense_log_var(x)\n",
    "    z = sampling([z_mean, z_log_var])\n",
    "\n",
    "    return Model(input_layer, [z_mean, z_log_var, z], name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder(\n",
    "    img_height,\n",
    "    img_width,\n",
    "    latent_dim=2,\n",
    "    ):\n",
    "    input_layer = Input(shape=(latent_dim,))\n",
    "    \n",
    "    # Two deconvolutions with stride 2, means out_dim = 2 * 2 * in_dim\n",
    "    in_height, in_width = img_height // 4, img_width // 4\n",
    "\n",
    "    dense1 = Dense(in_height * in_width * 64, activation=\"relu\")\n",
    "    reshape_layer = Reshape((in_height, in_width, 64))\n",
    "    conv1 = Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")\n",
    "    conv2 = Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")\n",
    "    conv3 = Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")\n",
    "\n",
    "    x = dense1(input_layer)\n",
    "    x = reshape_layer(x)\n",
    "    x = conv1(x)\n",
    "    x = conv2(x)\n",
    "    decoder_outputs = conv3(x)\n",
    "    return Model(input_layer, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = '../configs/debug_training_unet.json'\n",
    "config = read_json_config(CONFIG_PATH)\n",
    "parameters = config['parameters']\n",
    "\n",
    "results_dir = Path('..')\n",
    "\n",
    "# TF dimension ordering in this code\n",
    "K.set_image_data_format('channels_last')\n",
    "\n",
    "parameters = config['parameters']\n",
    "\n",
    "running_time = time.strftime('%b-%d-%Y_%H-%M')\n",
    "model_dir = results_dir / 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSOFRM_PATH = '../configs/transforms/0_5_fold_res/val.json'\n",
    "IMG_PATH = Path('../data/debug/img/')\n",
    "\n",
    "transform = load_albumentations_transform(TRANSOFRM_PATH)\n",
    "dataset = ImageRandomDataset(\n",
    "    IMG_PATH,\n",
    "    transform=transform,\n",
    "    batch_size=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = create_encoder(\n",
    "    img_height=parameters['target_height'],\n",
    "    img_width=parameters['target_width'],\n",
    "    latent_dim=16,\n",
    ")\n",
    "\n",
    "decoder = create_decoder(\n",
    "    img_height=parameters['target_height'],\n",
    "    img_width=parameters['target_width'],\n",
    "    latent_dim=16,\n",
    ")\n",
    "\n",
    "optimizer = Adam(ExponentialDecay(\n",
    "    initial_learning_rate = parameters['start_lr'],\n",
    "    decay_steps = parameters['samples_per_epoch']*parameters['scheduler']['step_size'],\n",
    "    decay_rate = parameters['scheduler']['gamma'],\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = VAETrainer(\n",
    "    encoder,\n",
    "    decoder,\n",
    "    optimizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "model = VAE(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0\n",
      "Train loss: 45307.78515625\n",
      "Val loss: 45252.95703125\n",
      "Epoch #1\n",
      "Train loss: 45248.8125\n",
      "Val loss: 45242.30078125\n",
      "Epoch #2\n",
      "Train loss: 45240.75390625\n",
      "Val loss: 45238.890625\n",
      "Epoch #3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      2\u001b[0m     \u001b[39m10\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     dataset,\n\u001b[1;32m      4\u001b[0m     dataset,\n\u001b[1;32m      5\u001b[0m )\n",
      "File \u001b[0;32m~/Projects/deeputi/src/models/trainer.py:89\u001b[0m, in \u001b[0;36mBaseTrainer.fit\u001b[0;34m(self, epochs, train_dataset, val_dataset)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_epoch_begin(epoch)\n\u001b[1;32m     87\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch #\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_epoch(Mode\u001b[39m.\u001b[39;49mTRAIN, train_dataset)\n\u001b[1;32m     90\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_loss \u001b[39m>\u001b[39m train_loss:\n",
      "File \u001b[0;32m~/Projects/deeputi/src/models/trainer.py:63\u001b[0m, in \u001b[0;36mBaseTrainer._run_epoch\u001b[0;34m(self, mode, dataset)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m Mode\u001b[39m.\u001b[39mTRAIN:\n\u001b[1;32m     62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_train_batch_begin(batch)\n\u001b[0;32m---> 63\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m     64\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mon_train_batch_end(batch)\n\u001b[1;32m     65\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/deeputi/src/models/trainer.py:187\u001b[0m, in \u001b[0;36mVAETrainer.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    184\u001b[0m     total_loss \u001b[39m=\u001b[39m reconstruction_loss \u001b[39m+\u001b[39m kl_loss\n\u001b[1;32m    186\u001b[0m trainable_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mtrainable_weights \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder\u001b[39m.\u001b[39mtrainable_weights\n\u001b[0;32m--> 187\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(total_loss, trainable_weights)\n\u001b[1;32m    188\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mapply_gradients(\u001b[39mzip\u001b[39m(grads, trainable_weights))\n\u001b[1;32m    190\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_loss_tracker\u001b[39m.\u001b[39mupdate_state(total_loss)\n",
      "File \u001b[0;32m~/micromamba/envs/deeputi/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:1112\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1106\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[1;32m   1107\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1108\u001b[0m           output_gradients))\n\u001b[1;32m   1109\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1110\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1112\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[1;32m   1113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[1;32m   1114\u001b[0m     flat_targets,\n\u001b[1;32m   1115\u001b[0m     flat_sources,\n\u001b[1;32m   1116\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[1;32m   1117\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[1;32m   1118\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[1;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[1;32m   1121\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/micromamba/envs/deeputi/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[0;32m~/micromamba/envs/deeputi/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    155\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[1;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/micromamba/envs/deeputi/lib/python3.10/site-packages/tensorflow/python/ops/nn_grad.py:55\u001b[0m, in \u001b[0;36m_Conv2DBackpropInputGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"The derivatives for deconvolution.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m  the gradients w.r.t. the input and the filter\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# functions for performance reasons in Eager mode. See _Conv2DGrad.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m     44\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     45\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39mconv2d_backprop_filter(\n\u001b[1;32m     46\u001b[0m         grad,\n\u001b[1;32m     47\u001b[0m         array_ops\u001b[39m.\u001b[39mshape(op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]),\n\u001b[1;32m     48\u001b[0m         op\u001b[39m.\u001b[39minputs[\u001b[39m2\u001b[39m],\n\u001b[1;32m     49\u001b[0m         dilations\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mdilations\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     50\u001b[0m         strides\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mstrides\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     51\u001b[0m         padding\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mpadding\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     52\u001b[0m         explicit_paddings\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mexplicit_paddings\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     53\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39muse_cudnn_on_gpu\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     54\u001b[0m         data_format\u001b[39m=\u001b[39mop\u001b[39m.\u001b[39mget_attr(\u001b[39m\"\u001b[39m\u001b[39mdata_format\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mdecode()),\n\u001b[0;32m---> 55\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39;49mconv2d(\n\u001b[1;32m     56\u001b[0m         grad,\n\u001b[1;32m     57\u001b[0m         op\u001b[39m.\u001b[39;49minputs[\u001b[39m1\u001b[39;49m],\n\u001b[1;32m     58\u001b[0m         dilations\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mdilations\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     59\u001b[0m         strides\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mstrides\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     60\u001b[0m         padding\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mpadding\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     61\u001b[0m         explicit_paddings\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mexplicit_paddings\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     62\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39muse_cudnn_on_gpu\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m     63\u001b[0m         data_format\u001b[39m=\u001b[39;49mop\u001b[39m.\u001b[39;49mget_attr(\u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mdecode())\n\u001b[1;32m     64\u001b[0m ]\n",
      "File \u001b[0;32m~/micromamba/envs/deeputi/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py:1101\u001b[0m, in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   1100\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   1102\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mConv2D\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, \u001b[39minput\u001b[39;49m, \u001b[39mfilter\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrides\u001b[39;49m\u001b[39m\"\u001b[39;49m, strides,\n\u001b[1;32m   1103\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39muse_cudnn_on_gpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_cudnn_on_gpu, \u001b[39m\"\u001b[39;49m\u001b[39mpadding\u001b[39;49m\u001b[39m\"\u001b[39;49m, padding,\n\u001b[1;32m   1104\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mexplicit_paddings\u001b[39;49m\u001b[39m\"\u001b[39;49m, explicit_paddings, \u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_format,\n\u001b[1;32m   1105\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mdilations\u001b[39;49m\u001b[39m\"\u001b[39;49m, dilations)\n\u001b[1;32m   1106\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   1107\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    10,\n",
    "    dataset,\n",
    "    dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
